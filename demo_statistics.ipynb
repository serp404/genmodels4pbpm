{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce77f1b",
   "metadata": {
    "cellId": "9mgmidtdgho1bdhbolgrq4"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c67056",
   "metadata": {
    "cellId": "hu8i0jvb4o34e7d77f0qv"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pm4py\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "random.seed(3407)\n",
    "torch.manual_seed(3407)\n",
    "torch.cuda.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7db95c",
   "metadata": {
    "cellId": "lsa5e9jvk92atddk2ey3v"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "LOG_TYPE = 'bpi12'\n",
    "event_log = pm4py.objects.log.importer.xes.importer.apply(f'./data/{LOG_TYPE}.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62389aa0",
   "metadata": {
    "cellId": "wiascg8elnor2pkdczxyl"
   },
   "source": [
    "### PM4PY log view\n",
    "\n",
    "В дальнейшем отфильтруем трейсы с крайне редкими окончаниями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb2452",
   "metadata": {
    "cellId": "6oevpo3r9jycousew3em"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "print(\"Log datetime:\", event_log[0][1]['time:timestamp'])\n",
    "print(\"Converting to ts\", int(event_log[0][1]['time:timestamp'].timestamp()))\n",
    "print()\n",
    "\n",
    "print(\"Trace view\")\n",
    "print(event_log[0])\n",
    "print()\n",
    "\n",
    "print(\"Event view\")\n",
    "print(event_log[0][1])\n",
    "print()\n",
    "\n",
    "print(\"Start activities\")\n",
    "print(pm4py.get_start_activities(event_log))\n",
    "print()\n",
    "\n",
    "print(\"End activities\")\n",
    "print(pm4py.get_end_activities(event_log))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e2f83",
   "metadata": {
    "cellId": "buhdwwjtg9r0v7qqkonv"
   },
   "source": [
    "### Traces lengths distribution\n",
    "\n",
    "В дальнейшем отфильтруем еще по длине (возьмем только трейсы длиннее 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9bf9e7",
   "metadata": {
    "cellId": "qtjgshz34lramxhs56kydl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "lens = [len(t) for t in event_log]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(lens, kde=True).grid()\n",
    "\n",
    "print(f'Median: {np.median(lens)}')\n",
    "print(f'Percentile 75: {np.percentile(lens, q=75)}')\n",
    "print(f'Percentile 90: {np.percentile(lens, q=90)}')\n",
    "print(f'Percentile 95: {np.percentile(lens, q=95)}')\n",
    "print(f'Percentile 99: {np.percentile(lens, q=99)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0974e58",
   "metadata": {
    "cellId": "dj32aw9ovjgspgheytns1o"
   },
   "source": [
    "### Traces durations distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26747c91",
   "metadata": {
    "cellId": "ixxm8h0pjtsp174aj2h3n"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "durs = pm4py.get_all_case_durations(event_log)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(durs, kde=True).grid()\n",
    "\n",
    "print(f'Median: {np.median(durs)}')\n",
    "print(f'Percentile 75: {np.percentile(durs, q=75)}')\n",
    "print(f'Percentile 90: {np.percentile(durs, q=90)}')\n",
    "print(f'Percentile 95: {np.percentile(durs, q=95)}')\n",
    "print(f'Percentile 99: {np.percentile(durs, q=99)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b795b",
   "metadata": {
    "cellId": "fde04vr69x86zuen3oduv7"
   },
   "source": [
    "### Activities frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfdfe51",
   "metadata": {
    "cellId": "tes7yr2hfknf3l3ondtr4j"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "activities = {}\n",
    "for t in event_log:\n",
    "    for e in t:\n",
    "        activities[e['concept:name']] = activities.get(e['concept:name'], 0) + 1\n",
    "\n",
    "for act_name, freq in sorted(activities.items(), key=lambda x: x[1]):\n",
    "    print(f\"Activity '{act_name}': {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff10fe8",
   "metadata": {
    "cellId": "5sbp129b0jmq8g0psht29"
   },
   "source": [
    "# Logs cleanup\n",
    "\n",
    "1. Фильтруем короткие и длинные трейсы (оставляем длинее 2 и короче 150)\n",
    "2. Фильтруем короткие по времени трейсы (оставляем длиннее 30 секунд)\n",
    "3. Фильтруем трейсы, которые заканчиваются на 'A_REGISTERED' или 'W_Wijzigen contractgegevens'\n",
    "4. Фильтруем трейсы, которые содержат 'W_Wijzigen contractgegevens'\n",
    "5. Делим на train, val, test по времени начала в хронологическом порядке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c35974",
   "metadata": {
    "cellId": "d143ofv2dybmegmx8sm2n"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from logmentations.datasets import filter_log\n",
    "\n",
    "event_log_filtered = filter_log(event_log, LOG_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6d89b",
   "metadata": {
    "cellId": "ksjq8ilybtn1foyytl0kkr"
   },
   "source": [
    "### Define mapping from activity name to activity id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad454638",
   "metadata": {
    "cellId": "ct45z7v1wjqohe15gt5h2"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "act2id = {'<PAD>': 0, '<BOS>': 1, '<EOS>': 2}\n",
    "id2act = {0: '<PAD>', 1: '<BOS>', 2: '<EOS>'}\n",
    "\n",
    "freqs = {}\n",
    "\n",
    "current_id = 3\n",
    "for t in event_log_filtered:\n",
    "    for e in t:\n",
    "        if e['concept:name'] not in act2id:\n",
    "            act2id[e['concept:name']] = current_id\n",
    "            id2act[current_id] = e['concept:name']\n",
    "            current_id += 1\n",
    "\n",
    "        freqs[act2id[e['concept:name']]] = freqs.get(act2id[e['concept:name']], 0) + 1\n",
    "\n",
    "events_cnt = sum(cnt for act, cnt in freqs.items())\n",
    "weights = {act: events_cnt / (2 * cnt) for act, cnt in freqs.items()}\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1ffc5",
   "metadata": {
    "cellId": "jhyhrsdrtg9ds1hbxpiqns"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# new lengths\n",
    "lens = [len(t) for t in event_log_filtered]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(lens, kde=True).grid()\n",
    "\n",
    "print(f'Median: {np.median(lens)}')\n",
    "print(f'Percentile 75: {np.percentile(lens, q=75)}')\n",
    "print(f'Percentile 90: {np.percentile(lens, q=90)}')\n",
    "print(f'Percentile 95: {np.percentile(lens, q=95)}')\n",
    "print(f'Percentile 99: {np.percentile(lens, q=99)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072d428",
   "metadata": {
    "cellId": "agx9aavgpnlv22j3wegq19"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# new durations\n",
    "durs = pm4py.get_all_case_durations(event_log_filtered)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(durs, kde=True).grid()\n",
    "\n",
    "print(f'Median: {np.median(durs)}')\n",
    "print(f'Percentile 75: {np.percentile(durs, q=75)}')\n",
    "print(f'Percentile 90: {np.percentile(durs, q=90)}')\n",
    "print(f'Percentile 95: {np.percentile(durs, q=95)}')\n",
    "print(f'Percentile 99: {np.percentile(durs, q=99)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977fb01",
   "metadata": {
    "cellId": "4z20ekrknv3lmafmxzcrlh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from logmentations.utils import uniform_kl\n",
    "\n",
    "# new frequencies\n",
    "activities = {}\n",
    "for t in event_log_filtered:\n",
    "    for e in t:\n",
    "        activities[act2id[e['concept:name']]] = activities.get(act2id[e['concept:name']], 0) + 1\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "print(activities)\n",
    "sns.barplot(\n",
    "    x=[p[0] for p in activities.items()],\n",
    "    y=[p[1] for p in activities.items()]\n",
    ").grid()\n",
    "\n",
    "initial_probs = np.array([val for _, val in activities.items()]) / sum(val for _, val in activities.items())\n",
    "print(f\"Initial uniformed KL: {uniform_kl(initial_probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f2cc8",
   "metadata": {
    "cellId": "tjkm2jh3hxsugwdcg0huh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from logmentations.utils import time_aware_data_split\n",
    "\n",
    "train_log, val_log, test_log = time_aware_data_split(event_log_filtered, (0.7, 0.1, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e16dd3",
   "metadata": {
    "cellId": "l9oijgpugdhd0kb4mggggs"
   },
   "source": [
    "# Log datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c780b47f",
   "metadata": {
    "cellId": "af7ay220swr011mquep1hzt"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from logmentations.datasets import LogsDataset\n",
    "\n",
    "normalizer_value = np.percentile(\n",
    "    [np.max(np.diff([t[i]['time:timestamp'].timestamp() for i in range(len(t))]))\n",
    "         for t in event_log_filtered], q=90\n",
    ")\n",
    "\n",
    "def time_scaling(time: float) -> float:\n",
    "    return time / normalizer_value\n",
    "\n",
    "train_ds = LogsDataset(train_log, act2id, time_applyer=time_scaling)\n",
    "val_ds = LogsDataset(val_log, act2id, time_applyer=time_scaling)\n",
    "test_ds = LogsDataset(test_log, act2id, time_applyer=time_scaling)\n",
    "\n",
    "print(f'Normalizer value: {normalizer_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe32354",
   "metadata": {
    "cellId": "1n2v0c3kuqarx2wijmmg3f"
   },
   "source": [
    "# Baseline augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af5382",
   "metadata": {
    "cellId": "1g1smzlfzfk1oncx0hu5j"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from logmentations.augmentations import StatisticsAugmentation\n",
    "\n",
    "aug = StatisticsAugmentation(act2id).fit(train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb805d",
   "metadata": {
    "cellId": "bt5v8ob43292sdkvvumd9x"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "rare_act_ids = [23, 25]\n",
    "# rare_act_ids = [21, 22, 25, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dcc96b",
   "metadata": {
    "cellId": "j7vbokzao09fdvker2ev7j"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "synthetic_traces = []\n",
    "for _ in tqdm.tqdm(range(50000), \"Sampling traces\"):\n",
    "    seq = aug.sample()\n",
    "    if any(map(lambda x: x[0] in rare_act_ids, seq)):\n",
    "        for i in range(len(seq)):\n",
    "            seq[i] = (seq[i][0], time_scaling(seq[i][1]))\n",
    "        synthetic_traces.append(seq)\n",
    "\n",
    "print(len(synthetic_traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1444cd68",
   "metadata": {
    "cellId": "l63xab8ua9qbnz3tkyuktd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "activities = {}\n",
    "for t in synthetic_traces:\n",
    "    for e in t:\n",
    "        activities[e[0]] = activities.get(e[0], 0) + 1\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "print(activities)\n",
    "sns.barplot(\n",
    "    x=[p[0] for p in activities.items()],\n",
    "    y=[p[1] for p in activities.items()]\n",
    ").grid()\n",
    "\n",
    "final_probs = np.array([val for _, val in activities.items()]) / sum(val for _, val in activities.items())\n",
    "print(f\"Final uniformed KL: {uniform_kl(final_probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8df36",
   "metadata": {
    "cellId": "8qgvv3n4ayyfn1jao6usd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from copy import deepcopy\n",
    "\n",
    "base_train_ds = deepcopy(train_ds)\n",
    "train_ds.extend_log(synthetic_traces)\n",
    "aug_train_ds = train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb758db",
   "metadata": {
    "cellId": "mddpw8re99vje50e4ai39"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from logmentations.utils import prediction_collate_fn\n",
    "from logmentations.datasets import SlicedLogsDataset\n",
    "from logmentations.datasets import LengthAwareSampler\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "sliced_base_train_ds = SlicedLogsDataset(base_train_ds)\n",
    "base_train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=sliced_base_train_ds,\n",
    "    batch_sampler=LengthAwareSampler(\n",
    "        data_len=len(sliced_base_train_ds),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        group_size=BATCH_SIZE * 16\n",
    "    ),\n",
    "    collate_fn=prediction_collate_fn,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "sliced_aug_train_ds = SlicedLogsDataset(aug_train_ds)\n",
    "aug_train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=sliced_aug_train_ds,\n",
    "    batch_sampler=LengthAwareSampler(\n",
    "        data_len=len(sliced_aug_train_ds),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        group_size=BATCH_SIZE * 16\n",
    "    ),\n",
    "    shuffle=False,\n",
    "    collate_fn=prediction_collate_fn,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=SlicedLogsDataset(val_ds),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=prediction_collate_fn,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    SlicedLogsDataset(test_ds),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=prediction_collate_fn,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f695d7",
   "metadata": {
    "cellId": "ry8lzusbqdlphvc1mem0j"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc76f3e",
   "metadata": {
    "cellId": "p64f3apicgq7arbwjesup6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from logmentations.models import LstmModel\n",
    "from logmentations.training import BaseConfig, train_predictive_epoch, eval_predictive_model\n",
    "\n",
    "model = LstmModel(\n",
    "    vocab_size=26, n_features=27,\n",
    "    emb_size=64, hid_size=128,\n",
    "    num_layers=3, bidirectional=True,\n",
    "    predict_time=True\n",
    ").to(DEVICE)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "SAVE_PERIOD = 25\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=N_EPOCHS)\n",
    "act_weight, time_weight = 1., 0.8\n",
    "\n",
    "CONFIG_BASE = BaseConfig({\n",
    "    \"n_epochs\": N_EPOCHS,\n",
    "    \"save_period\": SAVE_PERIOD,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"scheduler\": scheduler,\n",
    "    \"grad_clip_value\": 5.,\n",
    "    \"act_weight\": 1.,\n",
    "    \"time_weight\": 0.8,\n",
    "    \"device\": DEVICE\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687ec67",
   "metadata": {
    "cellId": "2hv0gs8mo57lxp8vgpcsrr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "run = wandb.init(\n",
    "    project=\"GenModels4PBPM-Prediction\",\n",
    "    entity=\"serp404\",\n",
    "    tags=[\"prediction\", \"origin_data\", LOG_TYPE]\n",
    ")\n",
    "\n",
    "save_path = os.path.join(\"./checkpoints\", run.name)\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "best_f1 = None\n",
    "for epoch in tqdm.notebook.tqdm(range(N_EPOCHS), \"Training\"):\n",
    "    # Train step\n",
    "    train_loss, train_ce, train_mae, grad_norm = train_predictive_epoch(\n",
    "        model, base_train_loader, CONFIG_BASE\n",
    "    )\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation step\n",
    "    val_loss, val_ce, val_mae, val_accuracy, val_f1_macro = eval_predictive_model(\n",
    "        model, val_loader, CONFIG_BASE\n",
    "    )\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_ce\": train_ce,\n",
    "            \"train_mae\": train_mae,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_ce\": val_ce,\n",
    "            \"val_mae\": val_mae,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_f1_macro_score\": val_f1_macro,\n",
    "            \"grad_norm\": grad_norm,\n",
    "            \"lr\": optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if best_f1 is None or val_f1_macro > best_f1:\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(save_path, f\"model_best.pth\")\n",
    "        )\n",
    "        best_f1 = val_f1_macro\n",
    "\n",
    "    if epoch % SAVE_PERIOD == 0:\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(save_path, f\"model_e{epoch}.pth\")\n",
    "        )\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6e6e0",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from logmentations.models import LstmModel\n",
    "from logmentations.training import eval_prediction_test_metrics\n",
    "\n",
    "# Test step\n",
    "model_best = LstmModel(\n",
    "    vocab_size=26, n_features=27,\n",
    "    emb_size=64, hid_size=128,\n",
    "    num_layers=3, bidirectional=True,\n",
    "    predict_time=True\n",
    ").to(DEVICE)\n",
    "\n",
    "model_best.load_state_dict(torch.load(os.path.join(save_path, \"model_best.pth\"), map_location=DEVICE))\n",
    "\n",
    "def time2days(time: float) -> float:\n",
    "    return invert_scaling(time) / 3600 / 24\n",
    "\n",
    "N_RUNS = 20\n",
    "loss, ce, inv_mae, accuracy, f1 = eval_prediction_test_metrics(\n",
    "    model_best, test_loader, CONFIG_BASE,\n",
    "    time2days=time2days, n_runs=N_RUNS\n",
    ")\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Loss CE: {ce}')\n",
    "print(f'Loss MAE: {inv_mae}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1-macro: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cf80b",
   "metadata": {
    "cellId": "ens5gvwlijqx7lsx0yx59"
   },
   "outputs": [],
   "source": [
    "#!g1.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "notebookId": "cef313ef-950b-488e-ac73-dcc37438083a",
  "notebookPath": "genmodels4pbpm-main/demo_statistics.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
